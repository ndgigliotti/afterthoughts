{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ndgig\\Repositories\\embed_gram\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import BatchEncoding\n",
    "\n",
    "os.chdir(\"C:\\\\Users\\\\ndgig\\\\Repositories\\\\embed_gram\")\n",
    "print(os.getcwd())\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizedDataset(Dataset):\n",
    "    def __init__(self, inputs):\n",
    "        self.inputs = inputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx].squeeze(0) for k, v in self.inputs.items()}\n",
    "\n",
    "def get_ngram_idx(input_ids, ngram_range=(3, 6)):\n",
    "    if isinstance(input_ids, torch.Tensor):\n",
    "        input_ids = input_ids.cpu().numpy()\n",
    "    ngram_idx = []\n",
    "    for ngram_size in range(ngram_range[0], ngram_range[1] + 1):\n",
    "        idx = np.vstack([np.arange(input_ids.shape[1]) + i for i in range(ngram_size)]).T\n",
    "        idx = idx[(idx[:, -1] < input_ids.shape[1])]\n",
    "        ngram_idx.append(idx)\n",
    "    return ngram_idx\n",
    "\n",
    "class NgramEncoder:\n",
    "    def __init__(self, model_name, device=\"cuda\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.model.eval().to(device)\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.model.device\n",
    "\n",
    "    def extract_ngrams(self, input_ids, token_embeds, ngram_range=(3, 6)):\n",
    "        ngrams = []\n",
    "        ngram_vecs = []\n",
    "        ngram_idx = get_ngram_idx(input_ids, ngram_range)\n",
    "        for idx in ngram_idx:\n",
    "            ngrams.append(input_ids[:, idx])\n",
    "            ngram_vecs.append(token_embeds[:, idx].mean(axis=2))\n",
    "        ngrams = [self.tokenizer.batch_decode(np.vstack(x)) for x in ngrams]\n",
    "        ngram_vecs = [np.vstack(x) for x in ngram_vecs]\n",
    "        ngrams = [y for x in ngrams for y in x]\n",
    "        ngram_vecs = np.vstack(ngram_vecs)\n",
    "        return ngrams, ngram_vecs\n",
    "    \n",
    "    def encode(self, docs, max_length=512, batch_size=32, amp=True, amp_dtype=torch.bfloat16):\n",
    "        if max_length is None:\n",
    "            if self.tokenizer.model_max_length is None:\n",
    "                raise ValueError(\n",
    "                    \"max_length must be specified if tokenizer.model_max_length is None\"\n",
    "                )\n",
    "            max_length = self.tokenizer.model_max_length\n",
    "            print(f\"max_length set to {max_length}\")\n",
    "        inputs = self.tokenizer(docs, max_length=max_length, padding=\"longest\", truncation=True, return_tensors=\"pt\")\n",
    "        loader = DataLoader(TokenizedDataset(inputs), batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "        outputs = []\n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast(enabled=amp, dtype=amp_dtype):\n",
    "                for batch in loader:\n",
    "                    batch = {k: v.to(self.device, non_blocking=True) for k, v in batch.items()}\n",
    "                    outputs.append(self.model(**batch).last_hidden_state.cpu().numpy())\n",
    "        outputs = np.concatenate(outputs, axis=0)\n",
    "        return self.extract_ngrams(inputs[\"input_ids\"], outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2],\n",
       "       [ 1,  2,  3],\n",
       "       [ 2,  3,  4],\n",
       "       [ 3,  4,  5],\n",
       "       [ 4,  5,  6],\n",
       "       [ 5,  6,  7],\n",
       "       [ 6,  7,  8],\n",
       "       [ 7,  8,  9],\n",
       "       [ 8,  9, 10],\n",
       "       [ 9, 10, 11],\n",
       "       [10, 11, 12],\n",
       "       [11, 12, 13],\n",
       "       [12, 13, 14],\n",
       "       [13, 14, 15],\n",
       "       [14, 15, 16],\n",
       "       [15, 16, 17],\n",
       "       [16, 17, 18],\n",
       "       [17, 18, 19],\n",
       "       [18, 19, 20],\n",
       "       [19, 20, 21],\n",
       "       [20, 21, 22],\n",
       "       [21, 22, 23],\n",
       "       [22, 23, 24],\n",
       "       [23, 24, 25],\n",
       "       [24, 25, 26],\n",
       "       [25, 26, 27],\n",
       "       [26, 27, 28],\n",
       "       [27, 28, 29],\n",
       "       [28, 29, 30],\n",
       "       [29, 30, 31],\n",
       "       [30, 31, 32],\n",
       "       [31, 32, 33],\n",
       "       [32, 33, 34],\n",
       "       [33, 34, 35],\n",
       "       [34, 35, 36],\n",
       "       [35, 36, 37],\n",
       "       [36, 37, 38],\n",
       "       [37, 38, 39],\n",
       "       [38, 39, 40],\n",
       "       [39, 40, 41],\n",
       "       [40, 41, 42],\n",
       "       [41, 42, 43],\n",
       "       [42, 43, 44],\n",
       "       [43, 44, 45],\n",
       "       [44, 45, 46],\n",
       "       [45, 46, 47],\n",
       "       [46, 47, 48],\n",
       "       [47, 48, 49],\n",
       "       [48, 49, 50],\n",
       "       [49, 50, 51],\n",
       "       [50, 51, 52],\n",
       "       [51, 52, 53],\n",
       "       [52, 53, 54],\n",
       "       [53, 54, 55],\n",
       "       [54, 55, 56],\n",
       "       [55, 56, 57],\n",
       "       [56, 57, 58],\n",
       "       [57, 58, 59],\n",
       "       [58, 59, 60],\n",
       "       [59, 60, 61],\n",
       "       [60, 61, 62],\n",
       "       [61, 62, 63],\n",
       "       [62, 63, 64],\n",
       "       [63, 64, 65],\n",
       "       [64, 65, 66],\n",
       "       [65, 66, 67],\n",
       "       [66, 67, 68],\n",
       "       [67, 68, 69],\n",
       "       [68, 69, 70],\n",
       "       [69, 70, 71],\n",
       "       [70, 71, 72],\n",
       "       [71, 72, 73],\n",
       "       [72, 73, 74],\n",
       "       [73, 74, 75],\n",
       "       [74, 75, 76]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack([np.arange(75) + i for i in range(3)]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ndgig\\miniforge3\\envs\\nlp1\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.NgramEncoder at 0x18a81d27200>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ng = NgramEncoder(\"microsoft/deberta-v3-small\")\n",
    "ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.419083  , -0.20553964,  0.20437278, ...,  0.0238938 ,\n",
       "         0.08819325, -0.22163458],\n",
       "       [ 0.40543842, -0.2113377 ,  0.2095865 , ...,  0.0425633 ,\n",
       "         0.08094309, -0.2358725 ],\n",
       "       [ 0.09142544, -0.32302827,  0.06221219, ...,  0.06342554,\n",
       "        -0.1965838 , -0.17689157],\n",
       "       ...,\n",
       "       [-0.21452475,  0.14686857,  0.34473768, ..., -0.02249772,\n",
       "        -0.14121933,  0.5197796 ],\n",
       "       [-0.39555085, -0.00161369,  0.21678615, ...,  0.09426519,\n",
       "        -0.05263494,  0.48606697],\n",
       "       [-0.5440827 , -0.04249418,  0.19302572, ...,  0.08400199,\n",
       "        -0.05081439,  0.30111313]], dtype=float32)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams, ngram_vecs = ng.encode([\"hello world\", \"goodbye world\", \"seriously goodbye\", \"oh no this one is much longer\"], batch_size=32)\n",
    "ngram_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['[CLS] hello world',\n",
       "  'hello world[SEP]',\n",
       "  'world[SEP][PAD]',\n",
       "  '[SEP][PAD][PAD]',\n",
       "  '[PAD][PAD][PAD]',\n",
       "  '[PAD][PAD][PAD]',\n",
       "  '[PAD][PAD][PAD]',\n",
       "  '[CLS] goodbye world',\n",
       "  'goodbye world[SEP]',\n",
       "  'world[SEP][PAD]',\n",
       "  '[SEP][PAD][PAD]',\n",
       "  '[PAD][PAD][PAD]',\n",
       "  '[PAD][PAD][PAD]',\n",
       "  '[PAD][PAD][PAD]',\n",
       "  '[CLS] seriously goodbye',\n",
       "  'seriously goodbye[SEP]',\n",
       "  'goodbye[SEP][PAD]',\n",
       "  '[SEP][PAD][PAD]',\n",
       "  '[PAD][PAD][PAD]',\n",
       "  '[PAD][PAD][PAD]',\n",
       "  '[PAD][PAD][PAD]',\n",
       "  '[CLS] oh no',\n",
       "  'oh no this',\n",
       "  'no this one',\n",
       "  'this one is',\n",
       "  'one is much',\n",
       "  'is much longer',\n",
       "  'much longer[SEP]',\n",
       "  '[CLS] hello world[SEP]',\n",
       "  'hello world[SEP][PAD]',\n",
       "  'world[SEP][PAD][PAD]',\n",
       "  '[SEP][PAD][PAD][PAD]',\n",
       "  '[PAD][PAD][PAD][PAD]',\n",
       "  '[PAD][PAD][PAD][PAD]',\n",
       "  '[CLS] goodbye world[SEP]',\n",
       "  'goodbye world[SEP][PAD]',\n",
       "  'world[SEP][PAD][PAD]',\n",
       "  '[SEP][PAD][PAD][PAD]',\n",
       "  '[PAD][PAD][PAD][PAD]',\n",
       "  '[PAD][PAD][PAD][PAD]',\n",
       "  '[CLS] seriously goodbye[SEP]',\n",
       "  'seriously goodbye[SEP][PAD]',\n",
       "  'goodbye[SEP][PAD][PAD]',\n",
       "  '[SEP][PAD][PAD][PAD]',\n",
       "  '[PAD][PAD][PAD][PAD]',\n",
       "  '[PAD][PAD][PAD][PAD]',\n",
       "  '[CLS] oh no this',\n",
       "  'oh no this one',\n",
       "  'no this one is',\n",
       "  'this one is much',\n",
       "  'one is much longer',\n",
       "  'is much longer[SEP]',\n",
       "  '[CLS] hello world[SEP][PAD]',\n",
       "  'hello world[SEP][PAD][PAD]',\n",
       "  'world[SEP][PAD][PAD][PAD]',\n",
       "  '[SEP][PAD][PAD][PAD][PAD]',\n",
       "  '[PAD][PAD][PAD][PAD][PAD]',\n",
       "  '[CLS] goodbye world[SEP][PAD]',\n",
       "  'goodbye world[SEP][PAD][PAD]',\n",
       "  'world[SEP][PAD][PAD][PAD]',\n",
       "  '[SEP][PAD][PAD][PAD][PAD]',\n",
       "  '[PAD][PAD][PAD][PAD][PAD]',\n",
       "  '[CLS] seriously goodbye[SEP][PAD]',\n",
       "  'seriously goodbye[SEP][PAD][PAD]',\n",
       "  'goodbye[SEP][PAD][PAD][PAD]',\n",
       "  '[SEP][PAD][PAD][PAD][PAD]',\n",
       "  '[PAD][PAD][PAD][PAD][PAD]',\n",
       "  '[CLS] oh no this one',\n",
       "  'oh no this one is',\n",
       "  'no this one is much',\n",
       "  'this one is much longer',\n",
       "  'one is much longer[SEP]',\n",
       "  '[CLS] hello world[SEP][PAD][PAD]',\n",
       "  'hello world[SEP][PAD][PAD][PAD]',\n",
       "  'world[SEP][PAD][PAD][PAD][PAD]',\n",
       "  '[SEP][PAD][PAD][PAD][PAD][PAD]',\n",
       "  '[CLS] goodbye world[SEP][PAD][PAD]',\n",
       "  'goodbye world[SEP][PAD][PAD][PAD]',\n",
       "  'world[SEP][PAD][PAD][PAD][PAD]',\n",
       "  '[SEP][PAD][PAD][PAD][PAD][PAD]',\n",
       "  '[CLS] seriously goodbye[SEP][PAD][PAD]',\n",
       "  'seriously goodbye[SEP][PAD][PAD][PAD]',\n",
       "  'goodbye[SEP][PAD][PAD][PAD][PAD]',\n",
       "  '[SEP][PAD][PAD][PAD][PAD][PAD]',\n",
       "  '[CLS] oh no this one is',\n",
       "  'oh no this one is much',\n",
       "  'no this one is much longer',\n",
       "  'this one is much longer[SEP]'],\n",
       " (88, 768))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams, ngram_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.419083  , -0.20553964,  0.20437278, ...,  0.0238938 ,\n",
       "         0.08819325, -0.22163458],\n",
       "       [ 0.40543842, -0.2113377 ,  0.2095865 , ...,  0.0425633 ,\n",
       "         0.08094309, -0.2358725 ],\n",
       "       [ 0.09142544, -0.32302827,  0.06221219, ...,  0.06342554,\n",
       "        -0.1965838 , -0.17689157],\n",
       "       ...,\n",
       "       [-0.21452475,  0.14686857,  0.34473768, ..., -0.02249772,\n",
       "        -0.14121933,  0.5197796 ],\n",
       "       [-0.39555085, -0.00161369,  0.21678615, ...,  0.09426519,\n",
       "        -0.05263494,  0.48606697],\n",
       "       [-0.5440827 , -0.04249418,  0.19302572, ...,  0.08400199,\n",
       "        -0.05081439,  0.30111313]], dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this one is much longer[SEP]',\n",
       " 'one is much longer[SEP]',\n",
       " 'this one is much longer',\n",
       " 'no this one is much longer',\n",
       " 'one is much longer']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=5, metric=\"cosine\")\n",
    "query = ngram_vecs[-1]\n",
    "nn.fit(ngram_vecs)\n",
    "dists, idx = nn.kneighbors([query])\n",
    "[ngrams[i] for i in idx[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
