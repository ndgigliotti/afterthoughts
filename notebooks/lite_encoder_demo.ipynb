{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LiteEncoder Demo on Real Dataset\n",
    "\n",
    "This notebook demonstrates how to use `LiteEncoder` from the `afterthoughts` library to generate sentence-chunk embeddings on a real dataset downloaded from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "from afterthoughts import LiteEncoder, configure_logging\n",
    "\n",
    "configure_logging(level=\"INFO\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "We'll use the AG News dataset, a popular news classification dataset with 4 categories: World, Sports, Business, and Sci/Tech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a subset of AG News for demonstration\n",
    "dataset = load_dataset(\"ag_news\", split=\"train[:1000]\")\n",
    "print(f\"Loaded {len(dataset)} documents\")\n",
    "print(f\"Columns: {dataset.column_names}\")\n",
    "print(f\"\\nExample document:\\n{dataset[0]['text'][:500]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the text documents\n",
    "docs = dataset[\"text\"]\n",
    "labels = dataset[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize LiteEncoder\n",
    "\n",
    "`LiteEncoder` is a memory-efficient variant that supports:\n",
    "- **Quantization options**: `\"float16\"` (2x) or `\"binary\"` (32x compression)\n",
    "- **PCA dimensionality reduction** (GPU-accelerated)\n",
    "- **Dimension truncation**\n",
    "\n",
    "We use `multi-qa-mpnet-base-dot-v1`, a model trained specifically for semantic search with questions and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LiteEncoder without PCA\n",
    "encoder = LiteEncoder(\n",
    "    model_name=\"sentence-transformers/multi-qa-mpnet-base-dot-v1\",\n",
    "    amp=True,  # Enable automatic mixed precision\n",
    "    quantize=\"float16\",  # Options: None, \"float16\" (2x), \"binary\" (32x)\n",
    "    normalize=True,  # Normalize embeddings to unit length\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    ")\n",
    "print(f\"Model loaded on device: {encoder.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Documents\n",
    "\n",
    "The `encode()` method extracts sentence-chunk embeddings from documents. Each chunk consists of groups of consecutive sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode documents with 2-sentence chunks\n",
    "df, embeddings = encoder.encode(\n",
    "    docs,\n",
    "    num_sents=2,  # Each chunk contains 2 consecutive sentences\n",
    "    chunk_overlap=0.5,  # 50% overlap between chunks (1 sentence)\n",
    "    batch_tokens=8192,  # Tokens per batch\n",
    "    return_frame=\"pandas\",\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(df)} chunks from {len(docs)} documents\")\n",
    "print(f\"Embedding shape: {embeddings.shape}\")\n",
    "print(f\"Embedding dtype: {embeddings.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the results dataframe\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10).style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Search Demo\n",
    "\n",
    "Let's demonstrate semantic search by encoding a query and finding the most similar chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode a query\n",
    "queries = [\n",
    "    \"stock market performance and financial news\",\n",
    "    \"sports championship results\",\n",
    "    \"technology innovation and AI\",\n",
    "]\n",
    "\n",
    "query_embeds = encoder.encode_queries(queries)\n",
    "print(f\"Query embedding shape: {query_embeds.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Build index using cosine similarity (since embeddings are normalized, this equals dot product)\n",
    "nn = NearestNeighbors(n_neighbors=5, metric=\"cosine\")\n",
    "nn.fit(embeddings)\n",
    "\n",
    "\n",
    "def semantic_search(query_embed, top_k=5):\n",
    "    \"\"\"Find top-k most similar chunks to a query.\"\"\"\n",
    "    distances, indices = nn.kneighbors([query_embed], n_neighbors=top_k)\n",
    "    # Convert cosine distance to similarity (1 - distance)\n",
    "    similarities = 1 - distances[0]\n",
    "\n",
    "    results = []\n",
    "    for idx, sim in zip(indices[0], similarities, strict=False):\n",
    "        results.append(\n",
    "            {\n",
    "                \"chunk\": df.iloc[idx][\"chunk\"],\n",
    "                \"document_idx\": df.iloc[idx][\"document_idx\"],\n",
    "                \"similarity\": sim,\n",
    "            }\n",
    "        )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for each query\n",
    "for i, query in enumerate(queries):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    results = semantic_search(query_embeds[i], top_k=3)\n",
    "\n",
    "    for j, result in enumerate(results, 1):\n",
    "        print(f\"\\n{j}. [Similarity: {result['similarity']:.4f}]\")\n",
    "        print(f\"   Doc #{result['document_idx']}: {result['chunk'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Chunk Sizes\n",
    "\n",
    "`LiteEncoder` can extract chunks of multiple sizes in a single pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode with multiple chunk sizes (1, 2, and 3 sentences per chunk)\n",
    "df_multi, embeddings_multi = encoder.encode(\n",
    "    docs[:100],  # Use fewer docs for demo\n",
    "    num_sents=[1, 2, 3],  # Multiple chunk sizes\n",
    "    chunk_overlap=0.5,\n",
    "    batch_tokens=8192,\n",
    "    return_frame=\"pandas\",\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(df_multi)} chunks with multiple sizes\")\n",
    "print(\"\\nChunk size distribution:\")\n",
    "print(df_multi[\"chunk_size\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View chunks of different sizes from the same document\n",
    "doc_0_chunks = df_multi[df_multi[\"document_idx\"] == 0][[\"chunk_idx\", \"chunk_size\", \"chunk\"]]\n",
    "print(\"Chunks from document 0:\")\n",
    "doc_0_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Efficiency\n",
    "\n",
    "Let's verify the memory savings from PCA and quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare memory usage with float16 quantization\n",
    "original_dim = 768  # multi-qa-mpnet-base-dot-v1 output dimension\n",
    "reduced_dim = embeddings.shape[1]\n",
    "\n",
    "original_bytes_per_embed = original_dim * 4  # float32\n",
    "reduced_bytes_per_embed = reduced_dim * 2  # float16\n",
    "\n",
    "num_embeds = len(embeddings)\n",
    "original_memory_mb = (num_embeds * original_bytes_per_embed) / (1024 * 1024)\n",
    "reduced_memory_mb = (num_embeds * reduced_bytes_per_embed) / (1024 * 1024)\n",
    "\n",
    "print(f\"Number of embeddings: {num_embeds:,}\")\n",
    "print(f\"Original (768 x float32): {original_memory_mb:.2f} MB\")\n",
    "print(f\"With float16 ({reduced_dim} x float16): {reduced_memory_mb:.2f} MB\")\n",
    "print(f\"Memory reduction: {(1 - reduced_memory_mb / original_memory_mb) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
