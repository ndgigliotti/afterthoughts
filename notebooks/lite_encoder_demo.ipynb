{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LiteEncoder Demo on Real Dataset\n",
    "\n",
    "This notebook demonstrates how to use `LiteEncoder` from the `afterthoughts` library to generate sentence-chunk embeddings on a real dataset downloaded from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "from afterthoughts import LiteEncoder, configure_logging\n",
    "\n",
    "configure_logging(level=\"INFO\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "We'll use the AG News dataset, a popular news classification dataset with 4 categories: World, Sports, Business, and Sci/Tech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a subset of AG News for demonstration\n",
    "dataset = load_dataset(\"ag_news\", split=\"train[:500]\")\n",
    "print(f\"Loaded {len(dataset)} documents\")\n",
    "print(f\"Columns: {dataset.column_names}\")\n",
    "print(f\"\\nExample document:\\n{dataset[0]['text'][:500]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the text documents\n",
    "docs = dataset[\"text\"]\n",
    "labels = dataset[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize LiteEncoder\n",
    "\n",
    "`LiteEncoder` is a memory-efficient variant that supports:\n",
    "- **Quantization options**: `\"float16\"` (2x), `\"int8\"` (4x), or `\"binary\"` (32x compression)\n",
    "- **PCA dimensionality reduction** (GPU-accelerated)\n",
    "- **Dimension truncation**\n",
    "\n",
    "We use `jina-embeddings-v2-base-en`, a RoPE-based model with 8,192 token context. This model was specifically tested in the [Late Chunking paper](https://arxiv.org/abs/2409.04701) and outperforms APE-based models (like E5, GTE) for documents exceeding 512 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LiteEncoder with PCA enabled\n",
    "encoder = LiteEncoder(\n",
    "    model_name=\"jinaai/jina-embeddings-v2-base-en\",  # 8k context, RoPE-based\n",
    "    model_dtype=torch.float32,\n",
    "    attn_implementation=\"eager\",\n",
    "    amp=True,  # Enable automatic mixed precision\n",
    "    quantize=\"float16\",  # Options: None, \"float16\" (2x), \"int8\" (4x), \"binary\" (32x)\n",
    "    pca=32,  # PCA dimensions must be < batch size (number of chunks per batch)\n",
    "    pca_early_stop=0.5,  # Fit PCA on first 50% of batches\n",
    "    normalize=True,  # Normalize embeddings to unit length\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    ")\n",
    "print(f\"Model loaded on device: {encoder.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Documents\n",
    "\n",
    "The `encode()` method extracts sentence-chunk embeddings from documents. Each chunk consists of groups of consecutive sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode documents with 2-sentence chunks\n",
    "df, embeddings = encoder.encode(\n",
    "    docs,\n",
    "    num_sents=2,  # Each chunk contains 2 consecutive sentences\n",
    "    chunk_overlap=0.5,  # 50% overlap between chunks (1 sentence)\n",
    "    batch_tokens=8192,  # Tokens per batch\n",
    "    return_frame=\"pandas\",\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(df)} chunks from {len(docs)} documents\")\n",
    "print(f\"Embedding shape: {embeddings.shape}\")\n",
    "print(f\"Embedding dtype: {embeddings.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the results dataframe\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10).style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Search Demo\n",
    "\n",
    "Let's demonstrate semantic search by encoding a query and finding the most similar chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode a query\n",
    "queries = [\n",
    "    \"stock market performance and financial news\",\n",
    "    \"sports championship results\",\n",
    "    \"technology innovation and AI\",\n",
    "]\n",
    "\n",
    "query_embeds = encoder.encode_queries(queries)\n",
    "print(f\"Query embedding shape: {query_embeds.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def semantic_search(query_embed, doc_embeds, df, top_k=5):\n",
    "    \"\"\"Find top-k most similar chunks to a query.\"\"\"\n",
    "    # Since embeddings are normalized, dot product = cosine similarity\n",
    "    similarities = np.dot(doc_embeds, query_embed)\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append(\n",
    "            {\n",
    "                \"chunk\": df.iloc[idx][\"chunk\"],\n",
    "                \"document_idx\": df.iloc[idx][\"document_idx\"],\n",
    "                \"similarity\": similarities[idx],\n",
    "            }\n",
    "        )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for each query\n",
    "for i, query in enumerate(queries):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    results = semantic_search(query_embeds[i], embeddings, df, top_k=3)\n",
    "\n",
    "    for j, result in enumerate(results, 1):\n",
    "        print(f\"\\n{j}. [Similarity: {result['similarity']:.4f}]\")\n",
    "        print(f\"   Doc #{result['document_idx']}: {result['chunk'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Chunk Sizes\n",
    "\n",
    "`LiteEncoder` can extract chunks of multiple sizes in a single pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear PCA state before re-encoding with different settings\n",
    "encoder.clear_pca()\n",
    "\n",
    "# Encode with multiple chunk sizes (1, 2, and 3 sentences per chunk)\n",
    "df_multi, embeddings_multi = encoder.encode(\n",
    "    docs[:100],  # Use fewer docs for demo\n",
    "    num_sents=[1, 2, 3],  # Multiple chunk sizes\n",
    "    chunk_overlap=0.5,\n",
    "    batch_tokens=8192,\n",
    "    return_frame=\"pandas\",\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(df_multi)} chunks with multiple sizes\")\n",
    "print(\"\\nChunk size distribution:\")\n",
    "print(df_multi[\"chunk_size\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View chunks of different sizes from the same document\n",
    "doc_0_chunks = df_multi[df_multi[\"document_idx\"] == 0][[\"chunk_idx\", \"chunk_size\", \"chunk\"]]\n",
    "print(\"Chunks from document 0:\")\n",
    "doc_0_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Efficiency\n",
    "\n",
    "Let's verify the memory savings from PCA and quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare memory usage\n",
    "original_dim = 768  # jina-embeddings-v2-base-en output dimension\n",
    "reduced_dim = embeddings.shape[1]\n",
    "\n",
    "original_bytes_per_embed = original_dim * 4  # float32\n",
    "reduced_bytes_per_embed = reduced_dim * 2  # float16\n",
    "\n",
    "num_embeds = len(embeddings)\n",
    "original_memory_mb = (num_embeds * original_bytes_per_embed) / (1024 * 1024)\n",
    "reduced_memory_mb = (num_embeds * reduced_bytes_per_embed) / (1024 * 1024)\n",
    "\n",
    "print(f\"Number of embeddings: {num_embeds:,}\")\n",
    "print(f\"Original (768 x float32): {original_memory_mb:.2f} MB\")\n",
    "print(f\"Reduced ({reduced_dim} x float16): {reduced_memory_mb:.2f} MB\")\n",
    "print(f\"Memory reduction: {(1 - reduced_memory_mb / original_memory_mb) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
