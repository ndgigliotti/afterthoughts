{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How ColBERTv2 Works\n",
    "\n",
    "ColBERTv2 is a **late-interaction** retrieval model that achieves a sweet spot between effectiveness and efficiency. This notebook explains the core concepts and demonstrates how it works under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Retrieval Spectrum\n",
    "\n",
    "There are three main approaches to neural retrieval:\n",
    "\n",
    "| Approach | Interaction | Efficiency | Effectiveness |\n",
    "|----------|-------------|------------|---------------|\n",
    "| **Bi-encoder** (Dense Retrieval) | None - single vector per doc | ⭐⭐⭐ Fast | ⭐ Good |\n",
    "| **Cross-encoder** | Full attention between Q & D | ⭐ Slow | ⭐⭐⭐ Best |\n",
    "| **ColBERT** (Late Interaction) | Token-level MaxSim | ⭐⭐ Medium | ⭐⭐ Better |\n",
    "\n",
    "ColBERT's key insight: **delay the interaction between query and document until after encoding**, but still allow fine-grained token-level matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ColBERTv2 model\n",
    "model_name = \"colbert-ir/colbertv2.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model hidden size: {model.config.hidden_size}\")\n",
    "print(\"ColBERTv2 projects to: 128 dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Concept 1: Token-Level Embeddings\n",
    "\n",
    "Unlike dense retrieval which produces **one vector per document**, ColBERT produces **one vector per token**.\n",
    "\n",
    "```\n",
    "Dense Retrieval:  \"The cat sat\" → [0.1, 0.2, ..., 0.8]  (single 768-dim vector)\n",
    "ColBERT:          \"The cat sat\" → [[0.1, ...], [0.3, ...], [0.2, ...]]  (3 × 128-dim vectors)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_colbert(texts, is_query=False):\n",
    "    \"\"\"Encode texts using ColBERT-style token embeddings.\n",
    "\n",
    "    ColBERTv2 uses special tokens:\n",
    "    - Queries: [Q] token prepended\n",
    "    - Documents: [D] token prepended\n",
    "    \"\"\"\n",
    "    # ColBERT uses [Q] and [D] markers, but we'll use [CLS] for simplicity\n",
    "    # The actual colbertv2.0 checkpoint handles this internally\n",
    "\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        # Get token embeddings from last hidden state\n",
    "        token_embeddings = outputs.last_hidden_state\n",
    "\n",
    "    # ColBERTv2 uses a linear projection to 128 dimensions\n",
    "    # The model checkpoint includes this projection layer\n",
    "    # Here we'll L2 normalize as ColBERT does\n",
    "    token_embeddings = F.normalize(token_embeddings, p=2, dim=-1)\n",
    "\n",
    "    return token_embeddings, inputs[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode a query and see the token-level embeddings\n",
    "query = \"What is machine learning?\"\n",
    "query_embeds, query_mask = encode_colbert([query], is_query=True)\n",
    "\n",
    "tokens = tokenizer.tokenize(query)\n",
    "print(f\"Query: '{query}'\")\n",
    "print(f\"Tokens: {['[CLS]'] + tokens + ['[SEP]']}\")\n",
    "print(f\"Embedding shape: {query_embeds.shape}\")\n",
    "print(f\"  -> {query_embeds.shape[1]} tokens x {query_embeds.shape[2]} dimensions each\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Concept 2: MaxSim (Maximum Similarity)\n",
    "\n",
    "The magic of ColBERT is in how it scores query-document pairs using **MaxSim**:\n",
    "\n",
    "1. For each query token, find its **maximum similarity** with any document token\n",
    "2. Sum these maximum similarities across all query tokens\n",
    "\n",
    "$$\\text{Score}(Q, D) = \\sum_{q \\in Q} \\max_{d \\in D} (q \\cdot d)$$\n",
    "\n",
    "This allows **soft matching**: a query token can match its best semantic partner in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxsim_score(query_embeds, doc_embeds, query_mask, doc_mask):\n",
    "    \"\"\"Compute ColBERT MaxSim score between query and document.\n",
    "\n",
    "    Args:\n",
    "        query_embeds: (1, num_query_tokens, dim)\n",
    "        doc_embeds: (1, num_doc_tokens, dim)\n",
    "        query_mask: (1, num_query_tokens)\n",
    "        doc_mask: (1, num_doc_tokens)\n",
    "\n",
    "    Returns:\n",
    "        Scalar score\n",
    "    \"\"\"\n",
    "    # Compute all pairwise similarities: (num_query_tokens, num_doc_tokens)\n",
    "    # Since embeddings are L2-normalized, dot product = cosine similarity\n",
    "    similarity_matrix = torch.matmul(query_embeds[0], doc_embeds[0].T)\n",
    "\n",
    "    # Mask out padding tokens in document\n",
    "    similarity_matrix = similarity_matrix.masked_fill(doc_mask[0].unsqueeze(0) == 0, float(\"-inf\"))\n",
    "\n",
    "    # MaxSim: for each query token, take max similarity across doc tokens\n",
    "    max_similarities = similarity_matrix.max(dim=-1).values  # (num_query_tokens,)\n",
    "\n",
    "    # Mask out padding tokens in query and sum\n",
    "    max_similarities = max_similarities * query_mask[0].float()\n",
    "    score = max_similarities.sum()\n",
    "\n",
    "    return score, similarity_matrix, max_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Score a query against two documents\n",
    "query = \"How do neural networks learn?\"\n",
    "doc1 = \"Neural networks learn by adjusting weights through backpropagation.\"\n",
    "doc2 = \"The weather today is sunny and warm.\"\n",
    "\n",
    "# Encode\n",
    "query_embeds, query_mask = encode_colbert([query])\n",
    "doc1_embeds, doc1_mask = encode_colbert([doc1])\n",
    "doc2_embeds, doc2_mask = encode_colbert([doc2])\n",
    "\n",
    "# Score\n",
    "score1, sim_matrix1, max_sims1 = maxsim_score(query_embeds, doc1_embeds, query_mask, doc1_mask)\n",
    "score2, sim_matrix2, max_sims2 = maxsim_score(query_embeds, doc2_embeds, query_mask, doc2_mask)\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(f\"Doc 1: '{doc1}'\")\n",
    "print(f\"Score: {score1.item():.3f}\\n\")\n",
    "print(f\"Doc 2: '{doc2}'\")\n",
    "print(f\"Score: {score2.item():.3f}\\n\")\n",
    "print(\"→ Doc 1 is ranked higher (more relevant)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing MaxSim\n",
    "\n",
    "Let's visualize how MaxSim works by showing which document tokens each query token matches with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_maxsim(query, doc, query_embeds, doc_embeds, query_mask, doc_mask):\n",
    "    \"\"\"Visualize which document tokens each query token matches.\"\"\"\n",
    "    score, sim_matrix, max_sims = maxsim_score(query_embeds, doc_embeds, query_mask, doc_mask)\n",
    "\n",
    "    query_tokens = [\"[CLS]\"] + tokenizer.tokenize(query) + [\"[SEP]\"]\n",
    "    doc_tokens = [\"[CLS]\"] + tokenizer.tokenize(doc) + [\"[SEP]\"]\n",
    "\n",
    "    # Pad tokens to match embedding length\n",
    "    while len(query_tokens) < sim_matrix.shape[0]:\n",
    "        query_tokens.append(\"[PAD]\")\n",
    "    while len(doc_tokens) < sim_matrix.shape[1]:\n",
    "        doc_tokens.append(\"[PAD]\")\n",
    "\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(f\"Doc:   '{doc}'\")\n",
    "    print(\"\\nMaxSim breakdown (query token → best matching doc token):\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for i, (q_tok, max_sim) in enumerate(zip(query_tokens, max_sims, strict=False)):\n",
    "        if query_mask[0, i] == 0:  # Skip padding\n",
    "            continue\n",
    "        best_doc_idx = sim_matrix[i].argmax().item()\n",
    "        best_doc_tok = doc_tokens[best_doc_idx]\n",
    "        print(f\"  {q_tok:15} → {best_doc_tok:15} (sim: {max_sim.item():.3f})\")\n",
    "\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Total MaxSim Score: {score.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize for the relevant document\n",
    "visualize_maxsim(\n",
    "    \"How do neural networks learn?\",\n",
    "    \"Neural networks learn by adjusting weights through backpropagation.\",\n",
    "    query_embeds,\n",
    "    doc1_embeds,\n",
    "    query_mask,\n",
    "    doc1_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize for the irrelevant document\n",
    "visualize_maxsim(\n",
    "    \"How do neural networks learn?\",\n",
    "    \"The weather today is sunny and warm.\",\n",
    "    query_embeds,\n",
    "    doc2_embeds,\n",
    "    query_mask,\n",
    "    doc2_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Concept 3: Efficient Retrieval with ColBERT\n",
    "\n",
    "The power of ColBERT is that document embeddings can be **precomputed and indexed**:\n",
    "\n",
    "1. **Offline**: Encode all documents, store token embeddings\n",
    "2. **Online**: Encode query, compute MaxSim against stored embeddings\n",
    "\n",
    "ColBERTv2 introduces optimizations:\n",
    "- **Residual compression**: Compress token embeddings using centroids\n",
    "- **Denoised supervision**: Better training with distillation\n",
    "- **Dimension reduction**: 128-dim embeddings (vs 768 for BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a mini retrieval system\n",
    "documents = [\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"Deep learning uses neural networks with many layers.\",\n",
    "    \"Python is a popular programming language for data science.\",\n",
    "    \"Transformers revolutionized natural language processing.\",\n",
    "    \"The stock market closed higher today.\",\n",
    "    \"Gradient descent optimizes neural network parameters.\",\n",
    "]\n",
    "\n",
    "# \"Index\" the documents (in practice, this would be stored on disk)\n",
    "print(\"Indexing documents...\")\n",
    "doc_embeddings = []\n",
    "doc_masks = []\n",
    "for doc in documents:\n",
    "    embeds, mask = encode_colbert([doc])\n",
    "    doc_embeddings.append(embeds)\n",
    "    doc_masks.append(mask)\n",
    "print(f\"Indexed {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, top_k=3):\n",
    "    \"\"\"Search documents using ColBERT MaxSim.\"\"\"\n",
    "    query_embeds, query_mask = encode_colbert([query])\n",
    "\n",
    "    scores = []\n",
    "    for doc_emb, doc_mask in zip(doc_embeddings, doc_masks, strict=False):\n",
    "        score, _, _ = maxsim_score(query_embeds, doc_emb, query_mask, doc_mask)\n",
    "        scores.append(score.item())\n",
    "\n",
    "    # Rank by score\n",
    "    ranked_indices = np.argsort(scores)[::-1]\n",
    "\n",
    "    print(f\"Query: '{query}'\\n\")\n",
    "    print(f\"Top {top_k} results:\")\n",
    "    for rank, idx in enumerate(ranked_indices[:top_k], 1):\n",
    "        print(f\"  {rank}. [{scores[idx]:.2f}] {documents[idx]}\")\n",
    "    return ranked_indices[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try some queries\n",
    "search(\"What is deep learning?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search(\"How are neural networks trained?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search(\"programming for AI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why ColBERT Works Well\n",
    "\n",
    "1. **Fine-grained matching**: Token-level embeddings capture nuanced semantics\n",
    "   - \"neural networks\" in query can match \"neural\" AND \"networks\" separately in doc\n",
    "   \n",
    "2. **Soft matching via MaxSim**: Each query term finds its best match\n",
    "   - Synonyms work: \"learn\" can match \"train\", \"optimize\", etc.\n",
    "   \n",
    "3. **Precomputable**: Documents encoded offline, only query encoding at search time\n",
    "   - Much faster than cross-encoders\n",
    "\n",
    "4. **Better than single-vector**: Multiple vectors capture more information\n",
    "   - Dense retrieval loses information by compressing to one vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ColBERTv2 Improvements\n",
    "\n",
    "ColBERTv2 (2022) improved on the original ColBERT (2020):\n",
    "\n",
    "| Feature | ColBERT v1 | ColBERTv2 |\n",
    "|---------|------------|----------|\n",
    "| Embedding dim | 128 | 128 |\n",
    "| Compression | None | Residual compression |\n",
    "| Training | In-batch negatives | Denoised supervision + distillation |\n",
    "| Index size | Large | ~6-10x smaller |\n",
    "| Effectiveness | Good | State-of-the-art |\n",
    "\n",
    "The residual compression works by:\n",
    "1. Learning centroids that capture common token embedding patterns\n",
    "2. Storing only the residual (difference from nearest centroid)\n",
    "3. Quantizing residuals to reduce storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "ColBERTv2's key innovations:\n",
    "\n",
    "1. **Late interaction**: Encode Q and D independently, interact via MaxSim\n",
    "2. **Token embeddings**: Preserve fine-grained information (not single vector)\n",
    "3. **MaxSim scoring**: Sum of max similarities enables soft matching\n",
    "4. **Efficient indexing**: Residual compression for practical deployment\n",
    "\n",
    "This makes ColBERTv2 an excellent choice when you need:\n",
    "- Better effectiveness than dense retrieval\n",
    "- Better efficiency than cross-encoders\n",
    "- Fine-grained semantic matching capabilities"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
