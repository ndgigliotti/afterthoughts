{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing max_chunk_tokens Feature with IMDB Reviews\n",
    "\n",
    "This notebook tests the new `max_chunk_tokens` and `split_long_sents` parameters added to the Encoder class, plus the **aligned lists** feature for experimenting with multiple chunk configurations.\n",
    "\n",
    "## Features to Test\n",
    "\n",
    "1. **max_chunk_tokens**: Greedy token-based chunking that accumulates sentences until token limit is reached\n",
    "2. **split_long_sents**: How to handle sentences that exceed the token limit\n",
    "   - `True`: Split long sentences at token boundaries\n",
    "   - `False`: Keep sentences intact even if they exceed the limit\n",
    "3. **Combination with max_chunk_sents**: \"At most N sentences AND at most M tokens\"\n",
    "4. **Aligned lists (NEW!)**: Pass lists to both parameters for multiple configurations\n",
    "   - `max_chunk_sents=[1,2,3]` and `max_chunk_tokens=[64,128,256]` creates 3 configs\n",
    "   - DataFrames include `max_chunk_sents` and `max_chunk_tokens` columns for filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "from datasets import load_dataset\n",
    "\n",
    "from afterthoughts import Encoder\n",
    "\n",
    "# Load IMDB dataset\n",
    "print(\"Loading IMDB dataset...\")\n",
    "dataset = load_dataset(\"imdb\", split=\"test[:100]\")  # Just 100 samples for testing\n",
    "docs = dataset[\"text\"]\n",
    "labels = dataset[\"label\"]\n",
    "\n",
    "print(f\"Loaded {len(docs)} reviews\")\n",
    "print(f\"\\nFirst review preview (first 500 chars):\\n{docs[0][:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Encoder\n",
    "\n",
    "Using a small model for fast testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "encoder = Encoder(model_name, normalize=True)\n",
    "\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Max length: {encoder.tokenizer.model_max_length}\")\n",
    "print(f\"Device: {encoder.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Baseline - Sentence-based chunking (original behavior)\n",
    "\n",
    "No token limit, just 2 sentences per chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test 1: Baseline sentence-based chunking (2 sentences per chunk)\")\n",
    "df1, embeds1 = encoder.encode(\n",
    "    docs,\n",
    "    max_chunk_sents=2,\n",
    "    chunk_overlap_sents=0,\n",
    ")\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "print(f\"Total chunks: {len(df1)}\")\n",
    "print(f\"Embeddings shape: {embeds1.shape}\")\n",
    "print(\"\\nDataFrame preview:\")\n",
    "print(df1.head(10))\n",
    "print(\"\\nChunk size distribution (num_sents):\")\n",
    "print(df1[\"num_sents\"].value_counts().sort(\"num_sents\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Token-based chunking with max_chunk_tokens only\n",
    "\n",
    "Greedy accumulation of sentences up to 128 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test 2: Token-based chunking (max 128 tokens, no sentence limit)\")\n",
    "df2, embeds2 = encoder.encode(\n",
    "    docs,\n",
    "    max_chunk_sents=None,  # No sentence limit\n",
    "    max_chunk_tokens=128,\n",
    "    chunk_overlap_sents=0,\n",
    "    split_long_sents=True,\n",
    ")\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "print(f\"Total chunks: {len(df2)}\")\n",
    "print(f\"Embeddings shape: {embeds2.shape}\")\n",
    "print(\"\\nDataFrame preview:\")\n",
    "print(df2.head(10))\n",
    "print(\"\\nChunk size distribution (num_sents):\")\n",
    "print(df2[\"num_sents\"].value_counts().sort(\"num_sents\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Combined limits (max_chunk_sents AND max_chunk_tokens)\n",
    "\n",
    "At most 3 sentences AND at most 100 tokens - whichever limit is hit first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test 3: Combined limits (max 3 sentences AND max 100 tokens)\")\n",
    "df3, embeds3 = encoder.encode(\n",
    "    docs,\n",
    "    max_chunk_sents=3,\n",
    "    max_chunk_tokens=100,\n",
    "    chunk_overlap_sents=0,\n",
    "    split_long_sents=True,\n",
    ")\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "print(f\"Total chunks: {len(df3)}\")\n",
    "print(f\"Embeddings shape: {embeds3.shape}\")\n",
    "print(\"\\nDataFrame preview:\")\n",
    "print(df3.head(10))\n",
    "print(\"\\nChunk size distribution (num_sents):\")\n",
    "print(df3[\"num_sents\"].value_counts().sort(\"num_sents\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: split_long_sents=False\n",
    "\n",
    "Keep long sentences intact even if they exceed max_chunk_tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test 4: split_long_sents=False (keep sentences intact)\")\n",
    "df4, embeds4 = encoder.encode(\n",
    "    docs,\n",
    "    max_chunk_sents=None,\n",
    "    max_chunk_tokens=128,\n",
    "    chunk_overlap_sents=0,\n",
    "    split_long_sents=False,  # Keep sentences intact\n",
    ")\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "print(f\"Total chunks: {len(df4)}\")\n",
    "print(f\"Embeddings shape: {embeds4.shape}\")\n",
    "print(\"\\nDataFrame preview:\")\n",
    "print(df4.head(10))\n",
    "print(\"\\nChunk size distribution (num_sents):\")\n",
    "print(df4[\"num_sents\"].value_counts().sort(\"num_sents\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5: With chunk overlap\n",
    "\n",
    "Test overlap with token-based chunking (must be integer overlap in sentences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test 5: Token-based chunking with overlap (1 sentence overlap)\")\n",
    "df5, embeds5 = encoder.encode(\n",
    "    docs,\n",
    "    max_chunk_sents=None,\n",
    "    max_chunk_tokens=128,\n",
    "    chunk_overlap_sents=1,  # Must be integer (number of sentences)\n",
    "    split_long_sents=True,\n",
    ")\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "print(f\"Total chunks: {len(df5)}\")\n",
    "print(f\"Embeddings shape: {embeds5.shape}\")\n",
    "print(\"\\nDataFrame preview:\")\n",
    "print(df5.head(10))\n",
    "print(\"\\nChunk size distribution (num_sents):\")\n",
    "print(df5[\"num_sents\"].value_counts().sort(\"num_sents\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 6: Multiple chunk sizes (list approach)\n",
    "\n",
    "Test multiple max_chunk_sents values to extract different granularities simultaneously.\n",
    "**NEW**: DataFrame now includes `max_chunk_sents` and `max_chunk_tokens` columns to track which configuration produced each chunk!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test 6: Multiple chunk sizes (1, 2, 3 sentences)\")\n",
    "df6, embeds6 = encoder.encode(\n",
    "    docs,\n",
    "    max_chunk_sents=[1, 2, 3],  # Extract 1-sent, 2-sent, and 3-sent chunks\n",
    "    chunk_overlap_sents=0,\n",
    ")\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "print(f\"Total chunks: {len(df6)}\")\n",
    "print(f\"Embeddings shape: {embeds6.shape}\")\n",
    "print(\"\\nDataFrame preview:\")\n",
    "print(df6.head(15))\n",
    "print(\"\\nChunk size distribution (num_sents):\")\n",
    "print(df6[\"num_sents\"].value_counts().sort(\"num_sents\"))\n",
    "\n",
    "print(\"\\n✅ NEW: The DataFrame now has 'max_chunk_sents' and 'max_chunk_tokens' columns!\")\n",
    "print(\"You can filter by configuration:\")\n",
    "print(\"\\nChunks with max_chunk_sents=1:\")\n",
    "# Cast to int for comparison since the column is Object dtype (can contain None)\n",
    "print(df6.filter(pl.col(\"max_chunk_sents\").cast(pl.Int64, strict=False) == 1).head(3))\n",
    "print(\"\\nChunks with max_chunk_sents=3:\")\n",
    "print(df6.filter(pl.col(\"max_chunk_sents\").cast(pl.Int64, strict=False) == 3).head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test 6.5: Aligned lists (NEW!)\")\n",
    "print(\"Testing: max_chunk_sents=[1, 2] and max_chunk_tokens=[64, 128]\")\n",
    "print(\"Expected: 2 configs (1,64) and (2,128) - NOT 4 configs!\")\n",
    "\n",
    "df6_5, embeds6_5 = encoder.encode(\n",
    "    docs[:20],  # Use fewer docs for speed\n",
    "    max_chunk_sents=[1, 2],\n",
    "    max_chunk_tokens=[64, 128],  # Same length - creates aligned pairs!\n",
    "    chunk_overlap_sents=0,\n",
    "    split_long_sents=True,\n",
    ")\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "print(f\"Total chunks: {len(df6_5)}\")\n",
    "print(f\"Embeddings shape: {embeds6_5.shape}\")\n",
    "\n",
    "# Show unique configurations\n",
    "print(\"\\n✅ Unique configurations (max_chunk_sents, max_chunk_tokens):\")\n",
    "configs = df6_5.select([\"max_chunk_sents\", \"max_chunk_tokens\"]).unique()\n",
    "print(configs)\n",
    "\n",
    "# Count chunks per configuration\n",
    "print(\"\\n✅ Chunk count per configuration:\")\n",
    "config_counts = (\n",
    "    df6_5.group_by([\"max_chunk_sents\", \"max_chunk_tokens\"])\n",
    "    .agg(pl.count().alias(\"count\"))\n",
    "    .sort([\"max_chunk_sents\", \"max_chunk_tokens\"])\n",
    ")\n",
    "print(config_counts)\n",
    "\n",
    "print(\"\\n✅ Filter chunks by specific configuration:\")\n",
    "print(\"\\nChunks with config (1, 64):\")\n",
    "# Cast for comparison since columns are Object dtype\n",
    "filtered_1_64 = df6_5.filter(\n",
    "    (pl.col(\"max_chunk_sents\").cast(pl.Int64, strict=False) == 1)\n",
    "    & (pl.col(\"max_chunk_tokens\").cast(pl.Int64, strict=False) == 64)\n",
    ")\n",
    "print(filtered_1_64.head(3))\n",
    "\n",
    "print(\"\\nChunks with config (2, 128):\")\n",
    "filtered_2_128 = df6_5.filter(\n",
    "    (pl.col(\"max_chunk_sents\").cast(pl.Int64, strict=False) == 2)\n",
    "    & (pl.col(\"max_chunk_tokens\").cast(pl.Int64, strict=False) == 128)\n",
    ")\n",
    "print(filtered_2_128.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 6.5: Aligned Lists (NEW Feature!)\n",
    "\n",
    "Test the new aligned lists feature: pass lists to BOTH `max_chunk_sents` and `max_chunk_tokens`.\n",
    "When both are lists, they must have the same length and are processed as **aligned pairs** (NOT cartesian product).\n",
    "\n",
    "Example: `[1,2,3]` x `[64,128,256]` creates 3 configs: (1,64), (2,128), (3,256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 7: Inspect specific chunks\n",
    "\n",
    "Look at actual chunk text to verify behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test 7: Inspect chunk text from token-based chunking\")\n",
    "\n",
    "# Get chunks from first document\n",
    "doc0_chunks = df2.filter(pl.col(\"document_idx\") == 0)\n",
    "\n",
    "print(f\"\\nDocument 0 has {len(doc0_chunks)} chunks\")\n",
    "print(\"\\nFirst 5 chunks:\")\n",
    "for i, row in enumerate(doc0_chunks.head(5).iter_rows(named=True)):\n",
    "    print(\n",
    "        f\"\\n--- Chunk {i} (document_idx={row['document_idx']}, chunk_idx={row['chunk_idx']}, num_sents={row['num_sents']}) ---\"\n",
    "    )\n",
    "    print(f\"{row['chunk'][:300]}...\")  # First 300 chars\n",
    "\n",
    "    # Count tokens to verify limit\n",
    "    token_ids = encoder.tokenizer.encode(row[\"chunk\"], add_special_tokens=False)\n",
    "    print(f\"Token count: {len(token_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 8: Verify edge cases\n",
    "\n",
    "Test with very small token limits to trigger sentence splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test 8: Very small token limit (30 tokens) to trigger splitting\")\n",
    "df8, embeds8 = encoder.encode(\n",
    "    docs[:10],  # Just 10 docs for speed\n",
    "    max_chunk_sents=None,\n",
    "    max_chunk_tokens=30,\n",
    "    chunk_overlap_sents=0,\n",
    "    split_long_sents=True,\n",
    ")\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "print(f\"Total chunks: {len(df8)}\")\n",
    "print(f\"Embeddings shape: {embeds8.shape}\")\n",
    "print(\"\\nChunk size distribution (num_sents):\")\n",
    "print(df8[\"num_sents\"].value_counts().sort(\"num_sents\"))\n",
    "\n",
    "# Show some example chunks\n",
    "print(\"\\nExample chunks (first 5):\")\n",
    "for i, row in enumerate(df8.head(5).iter_rows(named=True)):\n",
    "    token_count = len(encoder.tokenizer.encode(row[\"chunk\"], add_special_tokens=False))\n",
    "    print(f\"\\nChunk {i}: {row['num_sents']} sents, {token_count} tokens\")\n",
    "    print(f\"  {row['chunk'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 9: Semantic search with token-based chunks\n",
    "\n",
    "Test query encoding and similarity search with the token-based chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test 9: Semantic search with token-based chunks\")\n",
    "\n",
    "# Use token-based chunks from Test 2\n",
    "queries = [\n",
    "    \"great acting and cinematography\",\n",
    "    \"terrible plot and boring story\",\n",
    "    \"amazing special effects\",\n",
    "]\n",
    "\n",
    "query_embeds = encoder.encode_queries(queries)\n",
    "print(f\"Query embeddings shape: {query_embeds.shape}\")\n",
    "\n",
    "# Compute similarities (cosine similarity via dot product since normalized)\n",
    "similarities = query_embeds @ embeds2.T\n",
    "print(f\"Similarities shape: {similarities.shape}\")\n",
    "\n",
    "# Find top 3 chunks for each query\n",
    "for i, query in enumerate(queries):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    # Get top 3 indices\n",
    "    top_k = 3\n",
    "    top_indices = np.argsort(similarities[i])[::-1][:top_k]\n",
    "\n",
    "    for rank, idx in enumerate(top_indices, 1):\n",
    "        chunk_row = df2[idx]\n",
    "        similarity = similarities[i, idx]\n",
    "        print(f\"\\nRank {rank} (similarity: {similarity:.4f}):\")\n",
    "        print(f\"  Document: {chunk_row['document_idx'][0]}\")\n",
    "        print(f\"  Chunk: {chunk_row['chunk_idx'][0]}\")\n",
    "        print(\n",
    "            f\"  Sentiment: {'Positive' if labels[chunk_row['document_idx'][0]] == 1 else 'Negative'}\"\n",
    "        )\n",
    "        print(f\"  Text: {chunk_row['chunk'][0][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics\n",
    "\n",
    "Compare different chunking strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results = [\n",
    "    (\"Test 1: Sentence-based (2 sents)\", df1),\n",
    "    (\"Test 2: Token-based (128 tokens)\", df2),\n",
    "    (\"Test 3: Combined (3 sents & 100 tokens)\", df3),\n",
    "    (\"Test 4: Token-based (no split)\", df4),\n",
    "    (\"Test 5: Token-based (with overlap)\", df5),\n",
    "]\n",
    "\n",
    "for name, df in results:\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Total chunks: {len(df)}\")\n",
    "    print(f\"  Chunks per document (avg): {len(df) / len(docs):.2f}\")\n",
    "    print(f\"  Sentences per chunk (avg): {df['num_sents'].mean():.2f}\")\n",
    "    print(f\"  Unique documents: {df['document_idx'].n_unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Validation Tests\n",
    "\n",
    "Test that proper errors are raised for invalid parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test: Error handling for max_chunk_tokens > max_length\")\n",
    "\n",
    "try:\n",
    "    df_error, _ = encoder.encode(\n",
    "        docs[:5],\n",
    "        max_chunk_tokens=1000,  # Exceeds model max_length (512)\n",
    "        max_length=512,\n",
    "    )\n",
    "    print(\"ERROR: Should have raised ValueError!\")\n",
    "except ValueError as e:\n",
    "    print(f\"✓ Correctly raised ValueError: {e}\")\n",
    "\n",
    "print(\"\\nTest: Error handling for float chunk_overlap_sents with max_chunk_tokens\")\n",
    "try:\n",
    "    df_error, _ = encoder.encode(\n",
    "        docs[:5],\n",
    "        max_chunk_tokens=128,\n",
    "        chunk_overlap_sents=0.5,  # Float not allowed with token-based chunking\n",
    "    )\n",
    "    print(\"ERROR: Should have raised TypeError!\")\n",
    "except TypeError as e:\n",
    "    print(f\"✓ Correctly raised TypeError: {e}\")\n",
    "\n",
    "print(\"\\nTest: Error handling for max_chunk_sents=None without max_chunk_tokens\")\n",
    "try:\n",
    "    df_error, _ = encoder.encode(\n",
    "        docs[:5],\n",
    "        max_chunk_sents=None,  # None only valid with max_chunk_tokens\n",
    "        max_chunk_tokens=None,\n",
    "    )\n",
    "    print(\"ERROR: Should have raised ValueError!\")\n",
    "except ValueError as e:\n",
    "    print(f\"✓ Correctly raised ValueError: {e}\")\n",
    "\n",
    "print(\"\\nTest: Aligned lists with mismatched lengths (should fail)\")\n",
    "try:\n",
    "    df_error, _ = encoder.encode(\n",
    "        docs[:5],\n",
    "        max_chunk_sents=[1, 2, 3],  # 3 items\n",
    "        max_chunk_tokens=[64, 128],  # 2 items - mismatch!\n",
    "    )\n",
    "    print(\"ERROR: Should have raised ValueError!\")\n",
    "except ValueError as e:\n",
    "    print(f\"✓ Correctly raised ValueError: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done!\n",
    "\n",
    "All tests completed. Review the outputs above to verify the `max_chunk_tokens` and `split_long_sents` features are working correctly."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
